<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Linear Regression</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">My Stats Notebook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Describing Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="GraphicalSummaries.html">Graphical Summaries</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Models
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="MakingInference.html">Making Inference</a>
    </li>
    <li>
      <a href="BinomialTests.html">Binomial Tests</a>
    </li>
    <li>
      <a href="tTests.html">t Tests</a>
    </li>
    <li>
      <a href="WilcoxonTests.html">Wilcoxon Tests</a>
    </li>
    <li>
      <a href="ANOVA.html">ANOVA</a>
    </li>
    <li>
      <a href="Kruskal.html">Kruskal-Wallis</a>
    </li>
    <li>
      <a href="LinearRegression.html">Linear Regression</a>
    </li>
    <li>
      <a href="Non-Linear-Regression.html">Non-Linear Regression</a>
    </li>
    <li>
      <a href="LogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="ChiSquaredTests.html">Chi-Squared Tests</a>
    </li>
    <li>
      <a href="PermutationTests.html">Permutation Tests</a>
    </li>
  </ul>
</li>
<li>
  <a href="Glossary.html">Glossary</a>
</li>
<li>
  <a href="Articles.html">Articles</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Linear Regression</h1>

</div>


<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
</script>
<hr />
<p>Determine which explanatory variables have a significant effect on
the mean of the quantitative response variable.</p>
<hr />
<div id="simple-linear-regression"
class="section level3 tabset tabset-fade tabset-pills">
<h3 class="tabset tabset-fade tabset-pills">Simple Linear
Regression</h3>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYQuantX.png" width=58px;></p>
</div>
<p>Simple linear regression is a good analysis technique when the data
consists of a single quantitative response variable <span
class="math inline">\(Y\)</span> and a single quantitative explanatory
variable <span class="math inline">\(X\)</span>.</p>
<div id="overview" class="section level4">
<h4>Overview</h4>
<div style="padding-left:125px;">
<p>The mathematical model of simple linear regression is <span
class="math display">\[
  Y_i = \underbrace{\overbrace{\beta_0}^{\text{intercept}} +
\overbrace{\beta_1}^{\text{slope}} X_i \ }_{\text{regression relation}}
+ \epsilon_i
\]</span> where <span class="math inline">\(\epsilon_{i} \sim
N(0,\sigma^2)\)</span> is the error term.</p>
<p>This model is appropriate when five assumptions can be made.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear Relation</strong>: the regression relation between
<span class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> is linear.</p></li>
<li><p><strong>Normal Errors</strong>: the error terms are normally
distributed with a mean of zero.</p></li>
</ol>
<ul>
<li><a href="https://data.library.virginia.edu/normality-assumption/"
class="uri">https://data.library.virginia.edu/normality-assumption/</a></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><p><strong>Constant Variance</strong>: the variance of the error
terms is constant over all <span class="math inline">\(X\)</span>
values.</p></li>
<li><p><strong>Fixed X</strong>: the <span
class="math inline">\(X\)</span> values can be considered fixed and
measured without error.</p></li>
<li><p><strong>Independent Errors</strong>: the error terms are
independent.</p></li>
</ol>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab for details about
checking the regression assumptions.</p>
</div>
<p><strong>Hypotheses</strong></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}^{\quad \text{(most
common)}}\quad\quad
\]</span></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}^{\quad\text{(sometimes useful)}}
\]</span></p>
<p>If <span class="math inline">\(\beta_1 = 0\)</span>, then the model
reduces to <span class="math inline">\(Y_i = \beta_0 +
\epsilon_i\)</span>, claiming <span class="math inline">\(X\)</span>
does not improve our understanding of the mean of <span
class="math inline">\(Y\)</span> if the null hypothesis is true.</p>
<p>If <span class="math inline">\(\beta_0 = 0\)</span>, then the model
reduces to <span class="math inline">\(Y_i = \beta_1 X +
\epsilon_i\)</span>, claiming the average <span
class="math inline">\(Y\)</span>-value is <span
class="math inline">\(0\)</span> when <span
class="math inline">\(X=0\)</span>.</p>
<hr />
</div>
</div>
<div id="r-instructions" class="section level4">
<h4>R Instructions</h4>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p><code>mylm &lt;- lm(y ~ x, data=YourDataSet)</code> <strong>Perform
the Regression</strong></p>
<p><code>summary(mylm)</code> <strong>View the Hypothesis Test
Results</strong></p>
<p><code>plot(mylm, which=1:2)</code> <strong>Check Assumptions 1, 2,
and 3</strong></p>
<ul>
<li><code>mylm</code> is some name you come up with to store the results
of the <code>lm()</code> test. Note that <code>lm()</code> stands for
“linear model.”</li>
<li><code>y</code> must be a “numeric” vector of the quantitative
response variable.</li>
<li><code>x</code> is the explanatory variable. It can either be
quantitative (most usual) or qualitative.</li>
<li><code>YourDataSet</code> is the name of your data set.</li>
</ul>
<p>To add the regression line to a scatterplot, first make the
scatterplot, then use the command</p>
<p><code>abline(mylm)</code></p>
<p>Finally, note that the <code>mylm</code> object contains the
<code>names(mylm)</code> of</p>
<ul>
<li><code>mylm$coefficients</code> Contains two values. The first is the
estimated <span class="math inline">\(y\)</span>-intercept. The second
is the estimated slope.</li>
<li><code>mylm$residuals</code> Contains the residuals from the
regression in the same order as the actual dataset.</li>
<li><code>mylm$fitted.values</code> The values of <span
class="math inline">\(\hat{Y}\)</span> in the same order as the original
dataset.</li>
<li><code>mylm$...</code> several other things that will not be
explained here.</li>
</ul>
<hr />
</div>
</div>
<div id="python-instructions" class="section level4">
<h4>Python Instructions</h4>
<div style="padding-left:125px;">
<p><strong>Scipy</strong></p>
<p><code>scipy.stats.linregress(x, y=None)</code><br />
—- <a
href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html?highlight=linregress#scipy.stats.linregress">Docs</a></p>
<ul>
<li><strong>Notes</strong>
<ul>
<li>Everything in the results column can be treated as an attribute and
accessed in that way</li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="explanation" class="section level4">
<h4>Explanation</h4>
<div style="padding-left:125px;">
<div id="the-true-unknown-model-for-y_i" class="section level5">
<h5>The True (unknown) Model for <span
class="math inline">\(Y_i\)</span></h5>
<p>The mathematical model of simple linear regression is <span
class="math display">\[
  Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{\text{regression relation}}
+ \underbrace{\epsilon_i}_{\substack{\text{statistical} \\
\text{relation}}}
\]</span> where <span class="math inline">\(\epsilon_{i} \sim
N(0,\sigma^2)\)</span> is the error term.</p>
<p>The model has two elements to it, a regression relation <span
class="math inline">\(E\{Y\} = \beta_0 + \beta_1 X\)</span> and a
statistical (i.e., a random) relation <span
class="math inline">\(\epsilon\)</span>.</p>
<ul>
<li><p>The regression relation <span class="math inline">\(E\{Y\} =
\beta_0 + \beta_1 X\)</span> creates the line of regression where <span
class="math inline">\(\beta_0\)</span> is the <span
class="math inline">\(y\)</span>-intercept of the line and <span
class="math inline">\(\beta_1\)</span> is the slope of the line. The
regression relationship provides the average <span
class="math inline">\(Y\)</span>-value for a given <span
class="math inline">\(X\)</span>-value.</p></li>
<li><p>The statistical relation, the <span
class="math inline">\(\epsilon_i\)</span>, allows the individual points
to deviate from the line.</p></li>
</ul>
<p>Typically, the term “expected value” is used instead of the term
“average” as another way of saying the mean, or average value. The
notation for the expected value is <span
class="math inline">\(E\{Y\}\)</span>. Thus, <span
class="math display">\[
  E\{Y\} = \underbrace{\beta_0 + \beta_1 X}_{\text{regression relation}}
\]</span></p>
</div>
<div id="the-estimated-model-haty_i" class="section level5">
<h5>The Estimated Model <span
class="math inline">\(\hat{Y}_i\)</span></h5>
<p>An estimate of the unkown regression relation can be obtained from a
sample of data. The estimate of the regression relation is given by
<span class="math inline">\(\hat{Y}_i\)</span>, pronounced “<span
class="math inline">\(Y\)</span> hat sub <span
class="math inline">\(i\)</span>,” which is defined by the equation
<span class="math display">\[
  \hat{Y}_i = b_0 + b_1 X_i
\]</span> Here <span class="math inline">\(b_0\)</span> is the estimate
of the true <span class="math inline">\(y\)</span>-intercept <span
class="math inline">\(\beta_0\)</span>, and <span
class="math inline">\(b_1\)</span> is the estimate of the true slope
<span class="math inline">\(\beta_1\)</span>. Note that the <span
class="math inline">\(b\)</span>’s are sample statistics like <span
class="math inline">\(\bar{x}\)</span> and the <span
class="math inline">\(\beta\)</span>’s are population parameters like
<span class="math inline">\(\mu\)</span>. The <span
class="math inline">\(b\)</span>’s estimate the <span
class="math inline">\(\beta\)</span>’s.</p>
<p>The formula for the estimate of the regression line (the regression
relation) is <span class="math display">\[
  \hat{Y}_i = \underbrace{b_0 + b_1 X_i}_{\text{estimated relation}}
\]</span> Thus, <span class="math inline">\(\hat{Y}\)</span> is the
estimator of <span class="math inline">\(E\{Y\}\)</span>. So <span
class="math inline">\(\hat{Y}\)</span> is interpreted as the estimated
average <span class="math inline">\(Y\)</span>-value for any given <span
class="math inline">\(X\)</span>-value.</p>
</div>
<div id="putting-it-all-together" class="section level5">
<h5>Putting it all Together</h5>
<p>This graphic depicts the true, but typically unknown, regression
relation (dotted line). It also shows how a sample of data from the true
regression relation (points) can be used to obtain an estimated
regression equation (solid line) that is fairly close to the truth
(dotted line).</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="assumptions" class="section level5">
<h5>Assumptions</h5>
<p>There are five assumptions that should be met for the mathematical
model of simple linear regression to be appropriate.</p>
<ol style="list-style-type: decimal">
<li>The regression relation between <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span
class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span
class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered
fixed and measured without error.</li>
<li>The error terms are independent.</li>
</ol>
<p><br /></p>
</div>
<div id="check" class="section level5">
<h5>Checking the Assumptions</h5>
<p>Residuals are used to diagnose departures from the regression
assumptions. Residuals are the difference between the observed value of
<span class="math inline">\(Y\)</span> (the points) and the predicted,
or estimated value, <span class="math inline">\(\hat{Y}\)</span>.
Denoting a residual by <span class="math inline">\(r_i\)</span>, <span
class="math display">\[
  r_i = Y_i - \hat{Y}_i
\]</span> The residual <span class="math inline">\(r_i\)</span>
estimates the true error <span
class="math inline">\(\epsilon_i\)</span>.</p>
<div style="padding-left:15px;">
<p><br /></p>
<div id="residuals-versus-fitted-values-plot-checks-assumptions-1-and-3"
class="section level6">
<h6>Residuals versus Fitted-values Plot: Checks Assumptions 1 and 3</h6>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-2-1.png" width="144" />
</td>
<td width="75%">
<p>The linear relationship and constant variances assumptions can be
diagnosed using a residuals versus fitted-values plot. The fitted values
are the <span class="math inline">\(\hat{Y}_i\)</span>. The residuals
versus fitted-values plot compares the residual to the magnitude of the
fitted-value.</p>
<p>| <a href="./Analyses/ResidualsFittedPlot.html">Explanation</a> |</p>
</td>
</tr>
</table>
<p><br /></p>
</div>
<div id="q-q-plot-of-the-residuals-checks-assumption-2"
class="section level6">
<h6>Q-Q Plot of the Residuals: Checks Assumption 2</h6>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-3-1.png" width="144" />
</td>
<td width="75%">
<p>The normality of the error terms can be assessed by considering a
normal probability plot (Q-Q Plot) of the residuals. If the residuals
appear to be normal, then the error terms are also considered to be
normal. If the residuals do not appear to be normal, then the error
terms are also assumed to violate the normality assumption.</p>
<p>| <a href="./Analyses/NormalProbabilityPlots.html">Explanation</a>
|</p>
</td>
</tr>
</table>
<p><br /></p>
</div>
<div id="residuals-versus-order-plot-checks-assumption-5"
class="section level6">
<h6>Residuals versus Order Plot: Checks Assumption 5</h6>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-4-1.png" width="144" />
</td>
<td width="75%">
<p>When the data is collected in a specific order, or has some other
important ordering to it, then the independence of the error terms can
be assessed. This is typically done by plotting the residuals against
their order of occurrance. If any dramatic trends are visible in the
plot, then the independence assumption is violated.</p>
<p>| <a href="./Analyses/ResidualsOrderPlot.html">Explanation</a> |</p>
</td>
</tr>
</table>
</div>
</div>
<p><br /></p>
</div>
<div id="interpreting-the-model-parameters" class="section level5">
<h5>Interpreting the Model Parameters</h5>
<p>The interpretation of <span class="math inline">\(\beta_0\)</span> is
only meaningful if <span class="math inline">\(X=0\)</span> is in the
scope of the model. If <span class="math inline">\(X=0\)</span> is in
the scope of the model, then the intercept is interpreted as the <span
class="math inline">\(E\{Y\}\)</span> when <span
class="math inline">\(X=0\)</span>. The interpretation of <span
class="math inline">\(\beta_1\)</span> is the amount of increase (or
decrease) in the <span class="math inline">\(E\{Y\}\)</span> per unit
change in <span class="math inline">\(X\)</span>.</p>
</div>
<div id="infModelParam" class="section level5">
<h5>Inference for the Model Parameters</h5>
<p>Most inference in regression is focused on the slope, <span
class="math inline">\(\beta_1\)</span>. Recall that the interpretation
of <span class="math inline">\(\beta_1\)</span> is the amount of
increase (or decrease) in the expected value of <span
class="math inline">\(Y\)</span> per unit change in <span
class="math inline">\(X\)</span>. There are three main scenarios where
inference about the slope is of interest.</p>
<ol style="list-style-type: decimal">
<li><p>Determine if there is evidence of a meaningful linear
relationship in the data. If <span class="math inline">\(\beta_1 =
0\)</span>, then there is no relation between <span
class="math inline">\(X\)</span> and <span
class="math inline">\(E\{Y\}\)</span>. Hence we might be interested in
testing the hypotheses <span class="math display">\[
  H_0: \beta_1 = 0
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq 0
\]</span></p></li>
<li><p>Determine if the slope is greater, less than, or different from
some other hypothesized value. In this case, we would be interested in
using hypotheses of the form <span class="math display">\[
  H_0: \beta_1 = \beta_{10}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10}
\]</span> where <span class="math inline">\(\beta_{10}\)</span> is some
hypothesized number.</p></li>
<li><p>To provide a confidence interval for the true value of <span
class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p><br /></p>
<div style="padding-left:15px;">
<div id="tTests" class="section level6">
<h6>t Tests</h6>
<p>Before we discuss how to test the hypotheses listed above or
construct a confidence interval, we must understand the distribution of
the estimate <span class="math inline">\(b_1\)</span> of the parameter
<span class="math inline">\(\beta_1\)</span>. Since <span
class="math inline">\(b_1\)</span> is an estimate, it will vary from
sample to sample, even though the truth, <span
class="math inline">\(\beta_1\)</span>, remains fixed. It turns out that
the sampling distribution of <span class="math inline">\(b_1\)</span>
(where the <span class="math inline">\(X\)</span> values remain fixed
from study to study) is normal with mean and variance: <span
class="math display">\[
  \mu_{b_1} = \beta_1
\]</span> <span class="math display">\[
  \sigma^2_{b_1} = \frac{\sigma^2}{\sum(X_i-\bar{X})^2}
\]</span> Hence, an immediate choice of statistical test to test the
hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10}
\]</span> where <span class="math inline">\(\beta_{10}\)</span> can be
zero, or any other value, is a t test given by <span
class="math display">\[
  t = \frac{b_1 - \beta_{10}}{\sigma_{b_1}}
\]</span> where <span class="math inline">\(\sigma^2_{b_1} =
\frac{MSE}{\sum(X_i-\bar{X})^2}\)</span>. With quite a bit of work it
has been shown that <span class="math inline">\(t\)</span> is
distributed as a <span class="math inline">\(t\)</span> distribution
with <span class="math inline">\(n-2\)</span> degrees of freedom. The
nearly identical test statistic for testing <span
class="math display">\[
  H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_0 \neq \beta_{00}
\]</span> is given by <span class="math display">\[
  t = \frac{b_0 - \beta_{00}}{\sigma_{b_0}}
\]</span> where <span class="math inline">\(\sigma^2_{b_0} =
MSE\left[\frac{1}{n}+\frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]\)</span>.
This version of <span class="math inline">\(t\)</span> has also been
shown to be distributed as a <span class="math inline">\(t\)</span>
distribution with <span class="math inline">\(n-2\)</span> degrees of
freedom.</p>
<p>Creating a confidence interval for either <span
class="math inline">\(\beta_1\)</span> or <span
class="math inline">\(\beta_0\)</span> follows immediately from these
results using the formulas <span class="math display">\[
  b_1 \pm t^*_{n-2}\cdot \sigma_{b_1}
\]</span> <span class="math display">\[
  b_0 \pm t^*_{n-2}\cdot \sigma{b_0}
\]</span> where <span class="math inline">\(t^*_{n-2}\)</span> is the
critical value from a t distribution with <span
class="math inline">\(n-2\)</span> degrees of freedom corresponding to
the chosen confidence level.</p>
<p><br /></p>
</div>
<div id="Ftests" class="section level6">
<h6>F tests</h6>
<p>Another way to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10}  \quad\quad \text{or} \quad\quad H_0:
\beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} \quad\quad \ \ \quad \quad H_a: \beta_0
\neq \beta_{00}
\]</span> is with an <span class="math inline">\(F\)</span> Test. One
downside of the F test is that we cannot construct confidence intervals.
Another is that we can only perform two-sided tests, we cannot use
one-sided alternatives with an F test. The upside is that an <span
class="math inline">\(F\)</span> test is very general and can be used in
many places that a t test cannot.</p>
<p>In its most general form, the <span class="math inline">\(F\)</span>
test partitions the sums of squared errors into different pieces and
compares the pieces to see what is accounting for the most variation in
the data. To test the hypothesis that <span
class="math inline">\(H_0:\beta_1=0\)</span> against the alternative
that <span class="math inline">\(H_a: \beta_1\neq 0\)</span>, we are
essentially comparing two models against each other. If <span
class="math inline">\(\beta_1=0\)</span>, then the corresponding model
would be <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. If
<span class="math inline">\(\beta_1\neq0\)</span>, then the model
remains <span
class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. We call the
model corresponding to the null hypothesis the reduced model because it
will always have fewer parameters than the model corresponding to the
alternative hypothesis (which we call the full model). This is the first
requirement of the <span class="math inline">\(F\)</span> Test, that the
null model (reduced model) have fewer “free” parameters than the
alternative model (full model). To demonstrate what we mean by “free”
parameters, consider the following example.</p>
<p>Say we wanted to test the hypothesis that <span
class="math inline">\(H_0:\beta_1 = 2.5\)</span> against the alternative
that <span class="math inline">\(\beta_1\neq2.5\)</span>. Then the null,
or reduced model, would be <span
class="math inline">\(E\{Y_i\}=\beta_0+2.5X_i\)</span>. The alternative,
or full model, would be <span
class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. Thus, the
null (reduced) model contains only one “free” parameter because <span
class="math inline">\(\beta_1\)</span> has been fixed to be 2.5 and is
no longer free to be estimated from the data. The alternative (full)
model contains two “free” parameters, both are to be estimated from the
data. The null (reduced) model must contain fewer free parameters than
the alternative (full) model.</p>
<p>Once the null and alternative models have been specified, the General
Linear Test is performed by appropriately partitioning the squared
errors into pieces corresponding to each model. In the first example
where we were testing <span class="math inline">\(H_0:
\beta_1=0\)</span> against <span
class="math inline">\(H_a:\beta_1\neq0\)</span> we have the partition
<span class="math display">\[
  \underbrace{Y_i-\bar{Y}}_{Total} = \underbrace{\hat{Y}_i -
\bar{Y}}_{Regression} + \underbrace{Y_i-\hat{Y}_i}_{Error}
\]</span> The reason we use <span class="math inline">\(\bar{Y}\)</span>
for the null model is that <span class="math inline">\(\bar{Y}\)</span>
is the unbiased estimator of <span
class="math inline">\(\beta_0\)</span> for the null model, <span
class="math inline">\(E\{Y_i\} = \beta_0\)</span>. Thus we would compute
the following sums of squares: <span class="math display">\[
  SSTO = \sum(Y_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSR = \sum(\hat{Y}_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSE = \sum(Y_i-\hat{Y}_i)^2
\]</span> and note that <span class="math inline">\(SSTO = SSR +
SSE\)</span>. Important to note is that <span
class="math inline">\(SSTO\)</span> uses the difference between the
observations <span class="math inline">\(Y_i\)</span> and the null
(reduced) model. The <span class="math inline">\(SSR\)</span> uses the
diffences between the alternative (full) and null (reduced) model. The
<span class="math inline">\(SSE\)</span> uses the differences between
the observations <span class="math inline">\(Y_i\)</span> and the
alternative (full) model. From these we could set up a General <span
class="math inline">\(F\)</span> table of the form</p>
<table style="width:100%;">
<colgroup>
<col width="21%" />
<col width="21%" />
<col width="10%" />
<col width="23%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Sum Sq</th>
<th>Df</th>
<th>Mean Sq</th>
<th>F Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Error</td>
<td><span class="math inline">\(SSR\)</span></td>
<td><span class="math inline">\(df_R-df_F\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\)</span></td>
<td><span
class="math inline">\(\frac{SSR}{df_R-df_F}\cdot\frac{df_F}{SSE}\)</span></td>
</tr>
<tr class="even">
<td>Residual Error</td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(df_F\)</span></td>
<td><span class="math inline">\(\frac{SSE}{df_F}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total Error</td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(df_R\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<p><br /></p>
</div>
<div id="estMod" class="section level5">
<h5>Estimating the Model Parameters</h5>
<p>There are two approaches to estimating the parameters <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> in model (<span
class="math inline">\(\ref{model}\)</span>). The oldest and most
tradiational approach is using the idea of least squares. A more general
approach uses the idea of maximum likelihood. Fortunately, for simple
linear regression, the estimates for <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> obtained from either method are
identical. The estimates for the true parameter values <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> are typically denoted by <span
class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span>, respectively, and are given by</p>
<p><span class="math display">\[
  b_0 = \frac{1}{n}\left(\sum Y_i - b_1\sum X_i\right) = \bar{Y} -
b_1\bar{X}
\]</span> <span class="math display">\[
  b_1 = \frac{\sum(X_i - \bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}
\]</span></p>
<p>It is important to note that these estimates are entirely determined
from the observed data <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>. When the regression equation is
written using the estimates instead of the parameters, we use the
notation <span class="math inline">\(\hat{Y}\)</span>, which is the
estimator of <span class="math inline">\(E\{Y\}\)</span>. Thus, we write
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1 X_i
\end{equation}\]</span> which is directly comparable to the true, but
unknown values <span class="math display">\[\begin{equation}
  E\{Y_i\} = \beta_0 + \beta_1 X_i.
  \label{exp}
\end{equation}\]</span></p>
</div>
<div id="leastSquares" class="section level5">
<h5>Least Squares</h5>
<p>To estimate the model parameters <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> using least squares, we start by
defining the function <span class="math inline">\(Q\)</span> as the sum
of the squared errors, <span class="math inline">\(\epsilon_i\)</span>.
<span class="math display">\[
  Q = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1
X_i))^2
\]</span> It is important to note that the function <span
class="math inline">\(Q\)</span> is viewed as a function of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> and that the values of <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> are considered fixed given a particular
data set has been observed. Using calculus, we can take the partial
derivatives of <span class="math inline">\(Q\)</span> with respect to
both <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. <span class="math display">\[
  \frac{\partial Q}{\partial \beta_0} = -2\sum (Y_i - \beta_0 -
\beta_1X_i)
\]</span> <span class="math display">\[
  \frac{\partial Q}{\partial \beta_1} = -2\sum
X_i(Y_i-\beta_0-\beta_1X_i)
\]</span> Setting these partial derivatives to zero, and solving the
resulting system of equations provides the values of the parameters
which minimize <span class="math inline">\(Q\)</span> for a given set of
data. After all the calculations are completed we find the values of the
parameter estimators <span class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span> (of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>, respectively) are as stated
previously.</p>
</div>
<div id="mle" class="section level5">
<h5>Maximum Likelihood</h5>
<p>The idea of maximum likelihood estimation is opposite that of least
squares. Instead of choosing those values of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> which minime the least squares
<span class="math inline">\(Q\)</span> function, we choose the values of
<span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> which maximize the likelihood
function. The likelihood function is created by first determining the
joint distribution of the <span class="math inline">\(Y_i\)</span> for
all observations <span class="math inline">\(i=1,\ldots,n\)</span>. We
can do this rather simply by using the assumption that the errors, <span
class="math inline">\(\epsilon_i\)</span> are independently normally
distributed. When events are independent, their joint probability is
simply the product of their individual probabilities. Thus, if <span
class="math inline">\(f(Y_i)\)</span> denotes the probability density
function for <span class="math inline">\(Y_i\)</span>, then the joint
probability density for all <span class="math inline">\(Y_i\)</span>,
<span class="math inline">\(f(Y_1,\ldots,Y_n)\)</span> is given by <span
class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i)
\]</span> Since each <span class="math inline">\(Y_i\)</span> is assumed
to be normally distributed with mean <span class="math inline">\(\beta_0
+ \beta_1 X_i\)</span> and variance <span
class="math inline">\(\sigma^2\)</span> (see model (<span
class="math inline">\(\ref{model}\)</span>)) we have that <span
class="math display">\[
  f(Y_i) =
\frac{1}{\sqrt{2\pi}\sigma}\exp{\left[-\frac{1}{2}\left(\frac{Y_i-\beta_0-\beta_1X_i}{\sigma}\right)^2\right]}
\]</span> which provides the joint probability as <span
class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) =
\frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> The likelihood function <span class="math inline">\(L\)</span>
is then given by consider the <span class="math inline">\(Y_i\)</span>
and <span class="math inline">\(X_i\)</span> fixed and the parameters
<span class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\sigma^2\)</span> as the variables in the
function. <span class="math display">\[
  L(\beta_0,\beta_1,\sigma^2) =
\frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> Instead of taking partial derivatives of <span
class="math inline">\(L\)</span> directly (with respect to all
parameters) we take the partial derivatives of the <span
class="math inline">\(\log\)</span> of <span
class="math inline">\(L\)</span>, which is easier to work with. In a
similar, but more difficult calculation, to that of minimizing <span
class="math inline">\(Q\)</span>, we obtain the values of <span
class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span>, and <span
class="math inline">\(\sigma^2\)</span> which maximize the log of <span
class="math inline">\(L\)</span>, and which therefore maximize <span
class="math inline">\(L\)</span>. (This is not an obvious result, but
can be verified after some intense calculations.) The additional result
that maximimum likelihood estimation provides that the least squares
estimates did not give us is the estimate <span
class="math inline">\(\hat{\sigma}^2\)</span> of <span
class="math inline">\(\sigma^2\)</span>. <span class="math display">\[
  \hat{\sigma}^2 = \frac{\sum(Y_i-\hat{Y}_i)^2}{n}
\]</span></p>
</div>
<div id="varEst" class="section level5">
<h5>Estimating the Variance</h5>
<p>As shown previously, we can obtain estimates for the model parameters
<span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> with either least squares
estimation or maximum likelihood estimation. It turns out that these
estimates for <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> are nice in the sense that on
average they provide the correct estimate of the true parameter, i.e.,
they are unbiased estimators. On the other hand, the maximum likelihood
estimate <span class="math inline">\(\hat{\sigma}^2\)</span> of the
model variance <span class="math inline">\(\sigma^2\)</span> is a biased
estimator. It is consistently wrong in its estimates of <span
class="math inline">\(\sigma^2\)</span>. Without going into all the
details, <span class="math inline">\(\hat{\sigma}^2\)</span> is a biased
estimator of <span class="math inline">\(\sigma^2\)</span> because its
denominator needs to represent the degrees of freedom associated with
the numerator. Since <span class="math inline">\(\hat{Y}_i\)</span> in
the numerator of <span class="math inline">\(\hat{\sigma}^2\)</span> is
defined by <span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1X_i
  \label{hatY}
\end{equation}\]</span> it follows that two means, <span
class="math inline">\(\bar{X}\)</span> and <span
class="math inline">\(\bar{Y}\)</span>, must be estimated from the data
to obtain <span class="math inline">\(\hat{Y}_i\)</span>, see formulas
(<span class="math inline">\(\ref{bO}\)</span>) and (<span
class="math inline">\(\ref{bI}\)</span>) for details. Anytime a mean is
estimated from the data we lose a degree of freedom. Hence, the
denominator for <span class="math inline">\(\hat{\sigma}^2\)</span>
should be <span class="math inline">\(n-2\)</span> instead of <span
class="math inline">\(n\)</span>. Some incredibly long calculations will
show that the estimator <span class="math display">\[\begin{equation}
  s^2 = MSE = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2}
\end{equation}\]</span> is an unbiased estimator of <span
class="math inline">\(\sigma^2\)</span>. Here <span
class="math inline">\(MSE\)</span> stands for mean squared error, which
is the most obvious name for a formula that squares the errors <span
class="math inline">\(Y_i-\hat{Y}_i\)</span> then adds them up and
divides by their degrees of freedom. Similarly, we call the numerator
<span class="math inline">\(\sum(Y_i-\hat{Y}_i)^2\)</span> the sum of
the squared errors, denoted by <span class="math inline">\(SSE\)</span>.
It is also important to note that the errors are often denoted by <span
class="math inline">\(e_i = Y_i-\hat{Y}_i\)</span>. Putting this all
together we get the following equivalent statements for <span
class="math inline">\(MSE\)</span>. <span
class="math display">\[\begin{equation}
  s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2} =
\frac{\sum e_i^2}{n-2}
\end{equation}\]</span> As a final note, even though the <span
class="math inline">\(E\{MSE\} = \sigma^2\)</span>, <span
class="math inline">\(MSE\)</span> is an unbiased estimator of <span
class="math inline">\(\sigma^2\)</span>, it unfortunately isn’t true
that <span class="math inline">\(\sqrt{MSE}\)</span> is an unbiased
estimator of <span class="math inline">\(\sigma\)</span>. This presents
a few problems later on.</p>
<hr />
</div>
</div>
</div>
</div>
<div id="section" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a
href="./Analyses/BodyWeightSLR.html">bodyweight</a>, <a
href="./Analyses/carsSLR.html">cars</a></p>
</div>
<hr />
<div id="multiple-linear-regression"
class="section level3 tabset tabset-fade tabset-pills">
<h3 class="tabset tabset-fade tabset-pills">Multiple Linear
Regression</h3>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYMultX.png" width=108px;></p>
</div>
<p>Multiple regression allows for more than one explanatory variable to
be included in the modeling of the expected value of the quantitative
response variable <span class="math inline">\(Y_i\)</span>.</p>
<div id="overview-1" class="section level4">
<h4>Overview</h4>
<div style="padding-left:125px;">
<p>A typical multiple regression model is given by the equation <span
class="math display">\[
  Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_p
X_{pi} + \epsilon_i
\]</span> where <span class="math inline">\(\epsilon_i\sim
N(0,\sigma^2)\)</span>.</p>
<p>The coefficient <span class="math inline">\(\beta_j\)</span> is
interpreted as the change in the expected value of <span
class="math inline">\(Y\)</span> for a unit increase in <span
class="math inline">\(X_{j}\)</span>, holding all other variables
constant, for <span class="math inline">\(j=1,\ldots,p\)</span>.</p>
<p>See the <strong>Explanation</strong> tab for details about possible
hypotheses here.</p>
<hr />
</div>
</div>
<div id="r-instructions-1" class="section level4">
<h4>R Instructions</h4>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p>Everything is the same as in simple linear regression except that
more variables are allowed in the call to <code>lm()</code>.</p>
<p><code>mylm &lt;- lm(y ~ x1 + x2 + ... + xp + ..., data=YourDataSet)</code></p>
<ul>
<li><code>mylm</code> is some name you come up with to store the results
of the <code>lm()</code> test. Note that <code>lm()</code> stands for
“linear model.”</li>
<li><code>y</code> must be a “numeric” vector of the quantitative
response variable.</li>
<li><code>x1</code>, <code>x2</code>, <code>...</code>, <code>xp</code>
are the explanatory variables. These can either be quantitative or
qualitative. Note that R treats “numeric” variables as quantitative and
“character” or “factor” variables as qualitative. Further, when R thinks
a variable is qualitative, then it creates a set of dummy variables that
are each coded as 0,1 variables. It creates one fewer dummy variables
than levels of the original qualitative variable.</li>
<li><code>YourDataSet</code> is the name of your data set.</li>
<li><code>...</code> interactions are also allowed.</li>
</ul>
<hr />
</div>
</div>
<div id="explanation-1" class="section level4">
<h4>Explanation</h4>
<div style="padding-left:125px;">
<p>The extension of linear regression to multiple regression is fairly
direct yet very powerful. Multiple regression expands the simple
regression model to include more explanatory variables. These extra
variables are sometimes called <em>covariates</em>. Like simple
regression, multiple regression still only allows for a single
quantitative response variable.</p>
<div id="the-model" class="section level5">
<h5>The Model</h5>
<p>The multiple linear regression model is given by <span
class="math display">\[
  Y_i = \underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots +
\beta_p X_{pi}}_{E\{Y\}} + \epsilon_i
\]</span> where <span class="math inline">\(\epsilon_i\sim
N(0,\sigma^2)\)</span>. Thus, it is a direct extension of the simple
linear regression model to the scenario where more than one explanatory
variable can be included in the model.</p>
<div style="padding-left:15px; color:#a8a8a8;">
<p><strong>Note</strong>: Interactions, transformations of other
variables, and qualitative variables can all be included in the model.
For example, if a model included three explanatory variables, <span
class="math inline">\(X_1,X_2,X_3\)</span>, then <span
class="math inline">\(X_{3i}\)</span> could be defined to be the
interaction between <span class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_3\)</span>, i.e., <span
class="math inline">\(X_{3i} = X_{1i}\cdot X_{2i}\)</span>. If <span
class="math inline">\(X_3\)</span> was instead to represent a
qualitative variable with two levels, then we could use a 0 to represent
one level of the variable and a 1 to represent the other level. If we
had good reason to do so, we could even let <span
class="math inline">\(X_{3i} = X_{1i}^2\)</span> or <span
class="math inline">\(\log(X_{1i})\)</span> or some other transformation
of another <span class="math inline">\(X\)</span> variable.</p>
</div>
<p>Say we are interested in the price of a vehicle, particularly the
price of a Cadillac. Simple linear regression would just use the mileage
of the vehicle to predict the price. This will probably not be very
successful as different makes of Cadillacs vary widely in their prices.
However, if we include other explanatory variables in our model, like
model of the vehicle, we should be able to do very well at predicting
the price of a particular vehicle. (Certainly other variables like the
number of doors, engine size, automatic or manual transmission and so on
could also be valuable explanatory variables.)</p>
</div>
<div id="interpretation" class="section level5">
<h5>Interpretation</h5>
<p>The only change to interpretation from the simple linear regression
model is that each coefficient, <span
class="math inline">\(\beta_j\)</span> <span
class="math inline">\(j=1,\ldots,p\)</span>, represents the change in
the <span class="math inline">\(E\{Y\}\)</span> for a unit change in
<span class="math inline">\(X_j\)</span>, <em>holding all other
variables constant.</em></p>
<p><br /></p>
</div>
<div id="assumptions-1" class="section level5">
<h5>Assumptions</h5>
<p>The assumptions of multiple linear regression are nearly identical to
simple linear regression, with the addition of one new assumption.</p>
<ol style="list-style-type: decimal">
<li>The regression relation between <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span
class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span
class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered
fixed and measured without error.</li>
<li>The error terms are independent.</li>
<li>All important variables are included in the model.</li>
</ol>
<p><br /></p>
</div>
<div id="check" class="section level5">
<h5>Checking the Assumptions</h5>
<p>The process of checking assumptions is the same for multiple linear
regression as it is for simple linear regression, with the addition of
one more tool, the added variable plot. Added variable plots can be used
to determine if a new variable should be included in the model.</p>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-5-1.png" width="144" />
</td>
<td width="75%">
<p>Let <span class="math inline">\(X_{new}\)</span> be a new explanatory
variable that could be added to the current multiple regression model.
Plotting the residuals from the current linear regression against <span
class="math inline">\(X_{new}\)</span> allows us to determine if <span
class="math inline">\(X_{new}\)</span> has any information to add to the
current model. If there is a trend in the plot, then <span
class="math inline">\(X_{new}\)</span> should be added to the model. If
there is no trend in the plot, then the <span
class="math inline">\(X_{new}\)</span> should be left out.</p>
<p>| <a href="./Analyses/AddedVariablePlot.html">Explanation</a> |</p>
</td>
</tr>
</table>
<p><br /></p>
</div>
<div id="infModelParam" class="section level5">
<h5>Inference for the Model Parameters</h5>
<p>Inference in the multiple regression model can be for any of the
model coefficients, <span class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span>, <span
class="math inline">\(\ldots\)</span>, <span
class="math inline">\(\beta_p\)</span> or for several coefficients
simultaneously.</p>
<p><br /></p>
<div id="t-tests" class="section level6">
<h6>t Tests</h6>
<p>The most typical tests for multiple regression are t Tests for a
single coefficient. The hypotheses for these t Tests are written as
<span class="math display">\[
  H_0: \beta_j = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0
\]</span> Note that these hypotheses assume that all other variables
(and coefficients) are already in the model. The significance of the
single variable is thus assessed after accounting for the effect of all
other variables. If a t Test of a single coefficient is significant,
then that variable should remain in the model. If the t Test for a
single coefficient is not significant, then the other variables in the
model provide the same information that the variable being tested
provides. Removing it from the model may be appropriate. However,
whenever a single variable is removed from the model the other variables
can change in their significance.</p>
<p><br /></p>
</div>
<div id="f-tests" class="section level6">
<h6>F Tests</h6>
<p>Another approach to testing hypotheses about coefficients is to use
an F Test. The F Test allows a single test for any group of hypotheses
simultaneously.</p>
<p>The most commonly used F Test is the one given by the hypotheses
<span class="math display">\[
  H_0: \beta_0 = \beta_1 = \cdots = \beta_p = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0 \ \text{for at least one}\ j \in \{0,1,\ldots,p\}
\]</span> However, any subset of coefficients could be tested in a
similar way using a customized F Test. The details of how to do this are
somewhat involved and are beyond the scope of this class.</p>
<p><br /></p>
</div>
</div>
<div id="rsquared" class="section level5">
<h5>Assessing the Model Fit</h5>
<p>There are many measures of the quality of a regression model. One of
the most popular measurements is the <span
class="math inline">\(R^2\)</span> value (“R-squared”). The <span
class="math inline">\(R^2\)</span> value is a measure of the proportion
of variation of the <span class="math inline">\(Y\)</span>-variable that
is explained by the model. Specifically, <span class="math display">\[
  R^2 = \frac{\text{SSR}}{\text{SSTO}} =
1-\frac{\text{SSE}}{\text{SSTO}}
\]</span> The range of <span class="math inline">\(R^2\)</span> is
between 0 and 1. Values close to 1 imply a very good model. Values close
to 0 imply a very poor model.</p>
<p>One difficulty of <span class="math inline">\(R^2\)</span> in
multiple regression is that it will always get larger when more
variables are included in the regression model. Thus, in multiple linear
regression, it is best to make an adjustment to the <span
class="math inline">\(R^2\)</span> value to protect against this
difficulty. The value of the adjusted <span
class="math inline">\(R^2\)</span> is given by <span
class="math display">\[
  R^2_{adj} = 1 - \frac{(n-1)}{(n-p)}\frac{\text{SSE}}{\text{SSTO}}
\]</span> The interpretation of <span
class="math inline">\(R^2_{adj}\)</span> is essentially the same as the
interpretation of <span class="math inline">\(R^2\)</span>, with the
understanding that a correction has been made for the number of
parameters included in the model, <span
class="math inline">\((n-p)\)</span>.</p>
<p><br /> <br /></p>
<hr />
</div>
</div>
</div>
</div>
</div>
<div id="section-1" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a
href="./Analyses/CivicVsCorollaMLR.html">Civic Vs Corolla</a> <a
href="./Analyses/cadillacsMLR.html">cadillacs</a></p>
</div>
<hr />
<footer>
</footer>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
